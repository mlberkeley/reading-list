{
    "type": "topic",
    "name": "Systems for ML",
    "children": [
        {
            "type": "topic",
            "name": "Small Models",
            "children": [
                {
                    "type": "paper",
                    "flavor": "MobileNet",
                    "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
                    "authors": null,
                    "link": "https://arxiv.org/abs/1704.04861",
                    "relevance": 1
                },
                {
                    "type": "paper",
                    "flavor": "MobileNet-v2",
                    "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
                    "authors": null,
                    "link": "https://arxiv.org/abs/1801.04381",
                    "relevance": 1
                },
                {
                    "type": "paper",
                    "flavor": "MobileNet-v3",
                    "title": "Searching for MobileNetV3",
                    "authors": null,
                    "link": "https://arxiv.org/abs/1905.02244",
                    "relevance": 1
                },
                {
                    "type": "paper",
                    "flavor": "SqueezeNet",
                    "title": "SqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters and <0.5MB Model Size",
                    "authors": null,
                    "link": "https://arxiv.org/abs/1602.07360",
                    "relevance": 1
                },
                {
                    "type": "paper",
                    "flavor": "EfficientNet",
                    "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
                    "authors": null,
                    "link": "https://arxiv.org/abs/1905.11946",
                    "relevance": 1
                },
                {
                    "type": "paper",
                    "flavor": "SqueezeDet",
                    "title": "SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving",
                    "authors": null,
                    "link": "https://arxiv.org/abs/1612.01051",
                    "relevance": 1
                }
            ]
        },
        {
            "type": "topic",
            "name": "Quantization",
            "children": [
            ]
        },
        {
            "type": "topic",
            "name": "Model Serving",
            "children": [
                {
                    "type": "paper",
                    "flavor": "InferLine",
                    "title": "InferLine: Prediction Pipeline Provisioning and Management for Tight Latency Objectives",
                    "authors": null,
                    "link": "https://ucbrise.github.io/cs294-ai-sys-fa19/assets/preprint/inferline_draft.pdf",
                    "relevance": 1
                }
            ]
        },
        {
            "type": "topic",
            "name": "Graph Compilation",
            "children": [
                {
                    "type": "paper",
                    "flavor": "TVM",
                    "title": "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning",
                    "authors": null,
                    "link": "https://arxiv.org/abs/1802.04799",
                    "relevance": 1
                }
            ]
        },
        {
            "type": "topic",
            "name": "Distributed Training",
            "children": [
                {
                    "type": "topic",
                    "name": "Background",
                    "children": [
                        {
                            "type": "paper",
                            "flavor": "HogWild",
                            "title": "Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent",
                            "authors": null,
                            "link": "https://people.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf",
                            "relevance": 1
                        },
                        {
                            "type": "paper",
                            "flavor": "Parameter Server",
                            "title": "Scaling Distributed Machine Learning with the Parameter Server",
                            "authors": null,
                            "link": "https://www.cs.cmu.edu/~muli/file/parameter_server_osdi14.pdf",
                            "relevance": 1
                        },
                        {
                            "type": "paper",
                            "flavor": "AllReduce",
                            "title": "AllReduce Blog Post",
                            "authors": null,
                            "link": "http://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/",
                            "relevance": 1
                        },
                        {
                            "type": "paper",
                            "flavor": "Data Parallel vs Model Parallel",
                            "title": "Data Parallel vs Model Parallel (Wikipedia)",
                            "authors": null,
                            "link": "https://en.wikipedia.org/w/index.php?title=Data_parallelism&oldid=807618997#Data_Parallelism_vs._Model_Parallelism[4]",
                            "relevance": 1
                        },
                        {
                            "type": "paper",
                            "flavor": "Distributed Overview",
                            "title": "Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis",
                            "authors": null,
                            "link": "https://arxiv.org/abs/1802.09941",
                            "relevance": 1
                        }
                    ]
                },
                {
                    "type": "topic",
                    "name": "Data Parallelism",
                    "children": [
                        {
                            "type": "paper",
                            "flavor": "Horovod",
                            "title": "Horovod: Fast and Easy Distributed Deep Learning in TensorFlow",
                            "authors": null,
                            "link": "https://arxiv.org/abs/1802.05799",
                            "relevance": 1
                        }
                    ]
                },
                {
                    "type": "topic",
                    "name": "Model Parallelism",
                    "children": [
                        {
                            "type": "paper",
                            "flavor": "GPipe",
                            "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism",
                            "authors": null,
                            "link": "https://arxiv.org/abs/1811.06965",
                            "relevance": 1
                        },
                        {
                            "type": "paper",
                            "flavor": "PipeDream",
                            "title": "PipeDream: Fast and Efficient Pipeline Parallel DNN Training",
                            "authors": null,
                            "link": "https://cs.stanford.edu/~matei/papers/2019/sosp_pipedream.pdf",
                            "relevance": 1
                        }
                    ]
                },
                {
                    "type": "topic",
                    "name": "Some Tricks",
                    "children": [
                        {
                            "type": "paper",
                            "flavor": "Gradient Compression",
                            "title": "3LC: Lightweight and Effective Traffic Compression for Distributed Machine Learning",
                            "authors": null,
                            "link": "https://arxiv.org/abs/1802.07389",
                            "relevance": 1
                        },
                        {
                            "type": "paper",
                            "flavor": "Gradent Sparsification",
                            "title": "Gradient Sparsification for Communication-Efficient Distributed Optimization",
                            "authors": null,
                            "link": "https://arxiv.org/abs/1710.09854",
                            "relevance": 1
                        }
                    ]
                },
                {
                    "type": "topic",
                    "name": "ImageNet in ___ Minutes",
                    "children": [
                        {
                            "type": "paper",
                            "flavor": "",
                            "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour",
                            "authors": null,
                            "link": "https://arxiv.org/abs/1706.02677",
                            "relevance": 1
                        },
                        {
                            "type": "paper",
                            "flavor": "With Hierarchical All-Reduce",
                            "title": "Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes",
                            "authors": null,
                            "link": "https://arxiv.org/abs/1807.11205",
                            "relevance": 1
                        }
                    ]
                },
                {
                    "type": "topic",
                    "name": "Some Tools",
                    "children": [
                        {
                            "type": "paper",
                            "flavor": "",
                            "title": "Horovod",
                            "authors": null,
                            "link": "https://github.com/horovod/horovod",
                            "relevance": 1
                        },
                        {
                            "type": "paper",
                            "flavor": "",
                            "title": "BytePS",
                            "authors": null,
                            "link": "https://github.com/bytedance/byteps",
                            "relevance": 1
                        },
                        {
                            "type": "paper",
                            "flavor": "",
                            "title": "TensorFlow Distributed",
                            "authors": null,
                            "link": "https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy",
                            "relevance": 1
                        },
                        {
                            "type": "paper",
                            "flavor": "",
                            "title": "PyTorch Distributed DataParallel",
                            "authors": null,
                            "link": "https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel",
                            "relevance": 1
                        }
                    ]
                }
            ]
        },
        {
            "type": "topic",
            "name": "Low Memory",
            "children": [
                {
                    "type": "topic",
                    "name": "Training",
                    "children": [
                        {
                            "type": "paper",
                            "flavor": "RevNet",
                            "title": "The Reversible Residual Network: Backpropagation Without Storing Activations",
                            "authors": null,
                            "link": "https://arxiv.org/abs/1707.04585",
                            "relevance": 1
                        },
                        {
                            "type": "paper",
                            "flavor": "Gradient Checkpointing",
                            "title": "Low-Memory Neural Network Training: A Technical Report",
                            "authors": null,
                            "link": "https://arxiv.org/pdf/1904.10631.pdf",
                            "relevance": 1
                        }
                    ]
                }
            ]
        }
    ]
}
